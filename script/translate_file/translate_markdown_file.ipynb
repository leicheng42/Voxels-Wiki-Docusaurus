{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate a markdown file into Chinese\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import openai\n",
    "\n",
    "# 读取YAML文件\n",
    "with open('config.yaml', 'r') as yaml_file:\n",
    "    data = yaml.safe_load(yaml_file)\n",
    "\n",
    "# 读取配置\n",
    "access_token = data['access_token_list'][1]\n",
    "BASE_URL = data['BASE_URL'][1]\n",
    "\n",
    "# openai.api_key = \"这里填 access token，不是 api key\"\n",
    "openai.api_base = BASE_URL\n",
    "openai.api_key = access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# OpenAI GPT-2 tokenizer is the same as GPT-3 tokenizer\n",
    "# we use it to count the number of tokens in the text\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "with open(\"data/geometry_slovenian.tex\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Count the tokens in each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "847"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里为了测试，缩短文章篇幅\n",
    "chunks = text[:20000].split('\\n\\n')\n",
    "ntokens = []\n",
    "for chunk in chunks:\n",
    "    ntokens.append(len(tokenizer.encode(chunk)))\n",
    "max(ntokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that a double newline is a good separator in this case, in order not to break the flow of the text. Also no individual chunk is larger than 1500 tokens. The model we will use is text-davinci-002, which has a limit of 4096 tokens, so we don't need to worry about breaking the chunks down further.\n",
    "\n",
    "We will group the shorter chunks into chunks of around 1000 tokens, to increase the coherence of the text, and decrease the frequency of breaks within the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def group_chunks(chunks, ntokens, max_len=1000, hard_max_len=3000):\n",
    "    \"\"\"\n",
    "    Group very short chunks, to form approximately page long chunks.\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    cur_batch = \"\"\n",
    "    cur_tokens = 0\n",
    "    \n",
    "    # iterate over chunks, and group the short ones together\n",
    "    for chunk, ntoken in zip(chunks, ntokens):\n",
    "        # discard chunks that exceed hard max length\n",
    "        if ntoken > hard_max_len:\n",
    "            print(f\"Warning: Chunk discarded for being too long ({ntoken} tokens > {hard_max_len} token limit). Preview: '{chunk[:50]}...'\")\n",
    "            continue\n",
    "\n",
    "        # if room in current batch, add new chunk\n",
    "        if cur_tokens + 1 + ntoken <= max_len:\n",
    "            cur_batch += \"\\n\\n\" + chunk\n",
    "            cur_tokens += 1 + ntoken  # adds 1 token for the two newlines\n",
    "        # otherwise, record the batch and start a new one\n",
    "        else:\n",
    "            batches.append(cur_batch)\n",
    "            cur_batch = chunk\n",
    "            cur_tokens = ntoken\n",
    "            \n",
    "    if cur_batch:  # add the last batch if it's not empty\n",
    "        batches.append(cur_batch)\n",
    "        \n",
    "    return batches\n",
    "\n",
    "\n",
    "chunks = group_chunks(chunks, ntokens)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that adding a sample untranslated and translated first command, where only the content of the chapter name needs to be translated, helps to get more consistent results.\n",
    "\n",
    "The format of the prompt sent to the model consists of:\n",
    "1. A high level instruction to translate only the text, but not commands into the desired language\n",
    "2. A sample untranslated command, where only the content of the chapter name needs to be translated\n",
    "3. The chunk of text to be translated\n",
    "4. The translated sample command from 2, which shows the model the beginning of the translation process\n",
    "\n",
    "The expected output is the translated chunk of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1: Basics of Geometry\n",
      "The first two chapters deal with the history and axiomatic design of geometry. The consequences of the axioms of incidence, congruence, and parallelism are discussed in detail, while in the other two groups (axioms of order and continuity), the consequences are mostly not proven. Chapters three and four deal with the relation of the congruence of figures, the use of the triangle congruence theorems, and a circle. In the fifth chapter, vectors are defined. Thales's theorem of proportion is proven. Chapter six deals with isometries and their use. Their classification has been performed. Chapters 7 and 8 deal with similarity transformations, figure similarity relation, and area of figures. The ninth chapter presents the inversion. At the end of each chapter (except the introductory one) are exercises. Solutions and instructions can be found in the last, tenth chapter.\n",
      "\n",
      "The book contains 341 theorems, 247 examples, and 418 solved problems (28 of them from the IMO). In this sense, the book in front of you is at the same time a preparing guide for the IMO. For some well-known theorems and problems, brief historical remarks are given. That can help high school and college students better understand the development of geometry over the centuries.\n",
      "\n",
      "A lot of help in writing the book was selflessly offered to me by Prof. Roman Drstvenšek, who read the manuscript in its entirety. With his professional and linguistic comments, he made a great contribution to the final version of the book. In that work, he was assisted by Prof. Ana Kretič Mamič. I kindly thank both of them for the effort and time they have generously devoted to this book. I would especially like to thank Prof. Kristjan Kocbek, who read the partial manuscript and with his critical remarks contributed to the significant improvement of the book.\n",
      "\n",
      "I sincerely thank Prof. Gordana Kenda Kozinc, who read and proofread the introductory chapter, and thanks also to Prof. Alenka Brilej, who helped Roman with the language test with quite a few useful tips.\n",
      "\n",
      "I also thank Prof. Dr. Predrag Janičić, who wrote the wonderful software package GCLC for LaTeX. Almost all pictures in this book were made with this package.\n",
      "\n",
      "Last but not least, I would like to thank the students of the Bežigrad Grammar School, the 1st Grammar School in Celje, and the Brežice Grammar School for their inspiration and support. Students from these schools attended the renewed course of geometry which I have already taken before years in Belgrade.\n"
     ]
    }
   ],
   "source": [
    "def translate_chunk(chunk, engine='text-davinci-002',\n",
    "                    dest_language='English',\n",
    "                    sample_translation=(\"\\poglavje{Osnove Geometrije} \\label{osn9Geom}\", \"\\poglavje{The basics of Geometry} \\label{osn9Geom}\")\n",
    "                    ):\n",
    "    prompt = f'''Translate only the text from the following LaTeX document into {dest_language}. Leave all LaTeX commands unchanged\n",
    "    \n",
    "\"\"\"\n",
    "{sample_translation[0]}\n",
    "{chunk}\"\"\"\n",
    "\n",
    "{sample_translation[1]}\n",
    "'''\n",
    "    # response = openai.Completion.create(\n",
    "    #     prompt=prompt,\n",
    "    #     engine=engine,\n",
    "    #     temperature=0,\n",
    "    #     top_p=1,\n",
    "    #     max_tokens=1500,\n",
    "    # )\n",
    "    # result = response['choices'][0]['text'].strip()\n",
    "    result = get_completion(prompt)\n",
    "    result = result.replace('\"\"\"', '') # remove the double quotes, as we used them to surround the text\n",
    "    return result\n",
    "print(translate_chunk(chunks[2], engine='text-davinci-002', dest_language='English'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that this one chunk in particular translates only the text, but leaves LaTeX commands intact.\n",
    "\n",
    "Let's now translate all the chunks in the book - this will take 2-3 hours, as we're processing requests sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "10 / 10\n"
     ]
    }
   ],
   "source": [
    "dest_language = \"English\"\n",
    "\n",
    "translated_chunks = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(str(i+1) + \" / \" + str(len(chunks)))\n",
    "    # translate each chunk\n",
    "    translated_chunks.append(translate_chunk(chunk, engine='text-davinci-002', dest_language=dest_language))\n",
    "\n",
    "# join the chunks together\n",
    "result = '\\n\\n'.join(translated_chunks)\n",
    "\n",
    "# save the final result\n",
    "with open(f\"data/geometry_{dest_language}.tex\", \"w\") as f:\n",
    "    f.write(result)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
